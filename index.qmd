---
title: "Teaching and Learning Bayesian Statistics with {bayesrules}"
author: "Mine Dogucu, PhD <br> University of California Irvine"
date: "2024-12-11"
subtitle: "Talk at the Department of Statistics, University of Auckland"
footer: "mdogucu.github.io/auckland-24"
format: 
  revealjs:
    slide-number: true
    incremental: true
    theme: ["style.scss"]
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(echo = TRUE,
                      comment="")
```

```{r}
#| echo: false
#| message: false
library(tidyverse)
library(bayesrules)
library(rstanarm)
library(bayesplot)
library(kableExtra)
theme_set(theme_gray(base_size = 18))
```

## About Me

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-align: center
#| out-width: 100%
knitr::include_graphics("img/uc.png")
```

:::

::: {.column width="50%"}

- Faculty member in the Department of Statistics at University of California Irvine

- UC Irvine campus is located on the homelands of the Acjachemen and Tongva peoples


:::

::::

## About You

???

## About Bayesian Methods

:::: {.columns}

::: {.column width="50%"}

Frequentist

```{r}
#| echo: false
#| fig-align: center
#| out-width: 100%
knitr::include_graphics("img/freq-diagram.png")
```

:::

::: {.column width="50%"}

Bayesian 

```{r}
#| echo: false
#| fig-align: center
#| out-width: 100%
knitr::include_graphics("img/bayes-diagram.png")
```

:::

::::


# An Overview of Undegraduate Bayesian Education 

[Dogucu, M. & Hu, J. (2022) The Current State of Undergraduate Bayesian Education and Recommendations for the Future. The American Statistician, 74(2), 405-413.](https://www.tandfonline.com/doi/full/10.1080/00031305.2022.2089232)


## Majors

```{r}
#| echo: false
#| fig-align: center
knitr::include_graphics("img/majors.png")
```

## Prerequisites

```{r}
#| echo: false
#| fig-align: center
knitr::include_graphics("img/prereq.jpeg")
```

# Why Teach Bayesian Methods?
1. Intuition

## Hypothesis Testing

Suppose that during a recent doctor’s visit, you tested positive for a very rare disease. 

$H_0$: no disease  
$H_A$: disease

If you only get to ask the doctor one question, which would it be?

a. What’s the chance that I actually have the disease?  
b. If in fact I don’t have the disease, what’s the chance that I would’ve gotten this positive test result?

## Confidence Intervals

```{r}
#| echo: false
#| fig-align: center
knitr::include_graphics("img/confidence-intervals.png")
```

# Why Teach Bayesian Methods?
2 – Perfect blend of statistics and computing

## Metropolis-Hastings Algorithm

```{r}
#| echo: false
#| fig-align: center
knitr::include_graphics("img/mh.png")
```

# Why Teach Bayesian Methods?
3 – Bayesian methods are becoming more common

##

```{r}
#| echo: false
#| fig-align: center
knitr::include_graphics("img/andrews.png")
```

[Andrews, 2024](https://www.youtube.com/watch?v=PtLGnNvMC2o&ab_channel=TeachingandLearningMathematicsOnline)

# Bayes Rules!

## 

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-alt: A headshot of a woman with long blonde hair wearing a brownish yellow tshirt and a red and pink floral silk scarf wrapped around her neck.
#| out-width: 80%

knitr::include_graphics("img/alicia.jpg")
```

<center>
Alicia Johnson  
Macalester College  
[`r fontawesome::fa(name = "link")`](https://ajohns24.github.io/portfolio)
[`r fontawesome::fa(name = "github")`](https://github.com/ajohns24)

</center>

:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-align: center
#| fig-alt: A headshot of a man with short dark hair, and a dark moustache. He is wearing a blue button up shirt and dark gray jacket
#| out-width: 80%
knitr::include_graphics("img/miles.png")
```

<center>
Miles Ott  
Tubi  
[`r fontawesome::fa(name = "linkedin")`](https://www.linkedin.com/in/miles-ott/)
[`r fontawesome::fa(name = "github")`](https://github.com/MilesOtt)

</center>
:::

::::

##

```{r}
#| echo: false
#| fig-align: center
#| out-width: 25%
#| fig-alt: a hex shaped logo with shiny green-pink disco ball and purple starry background. There is text that says Bayes Rules!
knitr::include_graphics("img/bayes-rules-hex.png")
```

<center>

:::: {.columns}

::: {.column width="50%"}

<script src="https://use.fontawesome.com/releases/v5.15.1/js/all.js" data-auto-replace-svg="nest"></script>

<i class="fas fa-book fa-2x" aria-hidden="true" title="Book icon"></i>

[Bayes Rules! An Introduction to Bayesian Modeling with R](https://bayesrulesbook.com)

:::

::: {.column width="50%"}


<i class="fab fa-r-project fa-2x" aria-hidden="true" title="R logo"></i>

[{bayesrules}](https://www.github.com/bayes-rules/bayesrules)


:::

::::


</center>

## A quick example

Let $\pi$ be the proportion of spam emails where $\pi \in [0, 1]$.

. . .

What do you think $\pi$ is? How certain are you?

## Prior Model

:::: {.columns}

::: {.column width="50%"}

```{r fig.align='center', fig.height=4, fig.alt="X axis reads pi with values from 0 to 1. Y axis reads f of pi). The curve of the graph has a high peak on the y-axis when pi equals to 0.5. The distribution is symmentric."}
plot_beta(alpha = 4, beta = 4)
```

:::

::: {.column width="50%"}

```{r fig.align='center', fig.height=4, fig.alt="X axis reads pi with values from 0 to 1. Y axis reads f of pi). The curve of the graph has a high peak on the y-axis when pi equals to 0. The curve is a concave one and y is decreasing as pi is increasing."}
plot_beta(alpha = 1, beta = 10)
```

:::

::::

## Binomial Likelihood

```{r fig.align='center', fig.height=4, fig.alt="X axis reads pi with values from 0 to 1. Y axis reads l of pi given capital Y = y). The curve of the graph has a high peak on the y-axis when pi equals to 0.3. The y-values are almost zero when pi is greater than 0.75."}
plot_binomial_likelihood(y = 3, n = 10)
```


## Posterior Model

```{r fig.align='center', fig.height=4, fig.alt="X axis reads pi with values from 0 to 1. Y axis reads density. Three curves are shown labeled as prior, (scaled) likelihood, and posterior. The prior curve of the graph has a high peak on the y-axis when pi equals to 0. The curve is a concave one and y is decreasing as pi is increasing. The likelihood curve has a high peak on the y-axis when pi equals to 0.3. The y-values are almost zero when pi is greater than 0.75. The posterior sits between the prior and likelihood curves."}
plot_beta_binomial(alpha = 1, beta = 10, y = 3, n = 10)
```

## Target Audience of the Book

- Advanced Undergraduate Students in Statistics / Data Science Programs

- Equally trained learners

- Prior course/training in statistics is required

- Familiarity with probability, calculus, and tidyverse is recommended.

##

::: {.panel-tabset}

## Bayesian Foundations

:::: {.columns}

::: {.column width="50%"}
Bayes' Rule

The Beta-Binomial Bayesian Model

Balance and Sequentiality in Bayesian Analysis

Conjugate Families

:::

::: {.column width="50%"}

```{r echo=FALSE, fig.align='center', fig.height=3, fig.width=4, fig.alt="three curves on a single plot with no axis labeled. It is coloring scheme indicates its similarity to the previous plot with prior, scaled likelihood and posterior"}

plot_beta_binomial(1, 4, 13, 28) +
  theme(panel.grid = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.background = element_blank(),
        legend.position = "none") 
```
:::

::::


## Posterior Simulation & Analysis

:::: {.columns}

::: {.column width="50%"}

```{r echo=FALSE, fig.align='center', out.width="100%", fig.alt="A traceplot with no axis labels. Traceplots have thin vertical lines with varying lengths."}
knitr::include_graphics("img/unit2.png")
```

:::

::: {.column width="50%"}
Grid Approximation

The Metropolis-Hastings Algorithm

Posterior Estimation

Posterior Hypothesis Testing

Posterior Prediction
:::

::::

## Regression and Classification

:::: {.columns}

::: {.column width="50%"}

```{r echo=FALSE, fig.align='center', out.width="100%", fig.alt="A scatterplot with multiple regression lines passing through points. These regression lines are not all over the place, they are clustered with similar but varyin intercepts and slopes."}
knitr::include_graphics("img/unit3.png")
```

:::

::: {.column width="50%"}

Normal Regression 

Poisson and Negative Binomial Regression 

Logistic Regression  

Naive Bayes Classification

:::

::::

## Hierarchical Models

:::: {.columns}

::: {.column width="50%"}
```{r echo=FALSE, fig.align='center', out.width="100%", fig.alt="a figure showing hierarchy with a rectangle on top. With a set of arrows pointing downwards leading to a set of rectangles below which also have a set of arrows pointing downwards leading to a different set of rectangles."}
knitr::include_graphics("img/unit4.png")
```
:::

::: {.column width="50%"}
Normal hierarchical models without predictors

Normal hierarchical models with predictors

Non-Normal Hierarchical Regression & Classification
:::

::::

:::


# Pedagogical Approach

## Checking Intuition


```{r echo=FALSE, fig.align='center', out.width="80%", fig.alt="There are two ellipses at the top of the image. The first ellipse reads 'Prior: Only 40% of articles are fake'. The second ellipse reads 'Data: Exclamation points are more common among fake news'. There are two arrows each from the upper two ellipses leading to a third ellipse in the lower part of the image. The third ellipse reads 'Posterior: Is the article fake or not?'"}
knitr::include_graphics("img/fake-news-diagram.png")
```


##

Active learning with quizzes

. . .


Hands-on programming

. . .

Computing and Math Together

. . .

Compute for a Single Case then use built in functions



## Accessibility and Inclusion

```{r checklist, echo=FALSE}
options(knitr.kable.NA = '')
readxl::read_xlsx("accessibility-checklist.xlsx") %>%  
  slice(1:3) %>% 
  knitr::kable() 
  
```


##

```{r checklist2, echo=FALSE}

readxl::read_xlsx("accessibility-checklist.xlsx") %>%  
  slice(4:5) %>% 
  knitr::kable() 
  
```

##

```{r checklist3, echo=FALSE}

readxl::read_xlsx("accessibility-checklist.xlsx") %>%  
  slice(6:11) %>% 
  knitr::kable() 
  
```

#

[Dogucu, M., Johnson, A. A., & Ott, M. (2023). Framework for Accessible and Inclusive Teaching Materials for Statistics and Data Science Courses. Journal of Statistics and Data Science Education, 31(2), 144–150. https://doi.org/10.1080/26939169.2023.2165988](https://www.tandfonline.com/doi/full/10.1080/26939169.2023.2165988)

# R packages

#

:::{.font50}
`r fontawesome::fa(name = "github")` <a href = "http://github.com/bayes-rules/bayesrules">library(bayesrules)</a> 

:::

```{r eval=FALSE}
devtools::install_github("bayes-rules/bayesrules")
```

##

:::: {.columns}

::: {.column width="50%"}

```{r fig.align='center', fig.alt="X axis reads pi with values from 0 to 1. Y axis reads f of pi). The curve of the graph has a high peak on the y-axis when pi equals to 0.25. The y-values are almost zero when pi is greater than 0.70."}
plot_beta(alpha = 3, beta = 8)
```


:::

::: {.column width="50%"}

```{r fig.align='center', fig.alt="X axis reads pi with values from 0 to 1. Y axis reads f of pi). The curve of the graph has a high peak on the y-axis when pi equals to 0.9. The y-values are almost zero when pi is less than 0.50."}

plot_beta(alpha = 10, beta = 2)
```
:::

::::

##


 
```{r fig.align='center', fig.alt="X axis reads pi with values from 0 to 1. Y axis reads density. Three curves are shown labeled as prior, (scaled) likelihood, and posterior. The prior curve of the graph has a high peak on the y-axis when pi equals to 0.25 and y values are close to zero when pi is greater than 0.7. The likelihood curve has a high peak on the y-axis when pi equals to 0.95 and is quite peaked with low variance. The posterior sits between the prior and likelihood curves."}
#| fig-align: center
plot_beta_binomial(alpha = 3, beta = 8, y = 19, n = 20)
```

## 

```{r, echo=FALSE, message=FALSE, cache=TRUE, fig.alt="3 by 3 prior, scaled likelihood, and posterior plots. In the first column of the plots data (thus the likelihood changes)  from 6 successes out of 13 trials to 29 out of 63 and then to 46 out of 99. All the likelihood curves have a similar pi value when y reaches its maximum. As sample size increases the posterior is closer and closer to the likelihood. Each row represents a different prior model including Beta(14, 1), Beta(5,11) and Beta(1,1) respectively from least peaked (informative) to flat. As the prior gets more informative (peaked) the posterior is closer to prior."}
## Code for facet_wrapped Beta-Binomial plots
### Plotting function
beta_binom_grid_plot <- function(data, likelihood = FALSE, posterior = FALSE){
  g <- ggplot(data, aes(x = pi, y = prior)) + 
    geom_line() + 
    geom_area(alpha = 0.5, aes(fill = "prior", x = pi, y = prior)) + 
    scale_fill_manual("", values = c(prior = "#f0e442", 
      `(scaled) likelihood` = "#0071b2", posterior = "#009e74"), breaks = c("prior", "(scaled) likelihood", "posterior")) + 
    labs(x = expression(pi), y = "density") + 
    theme(legend.position="bottom")
  
  if(likelihood == TRUE){
    g <- g + 
      geom_line(aes(x = pi, y = likelihood)) + 
      geom_area(alpha = 0.5, aes(fill = "(scaled) likelihood", x = pi, y = likelihood))
  }
  
  if(posterior == TRUE){
    g <- g + 
      geom_line(aes(x = pi, y = posterior)) + 
      geom_area(alpha = 0.5, aes(fill = "posterior", x = pi, y = posterior)) 
  }
  g
}
make_plot_data <- function(as, bs, xs, ns, labs_prior, labs_likelihood){
  ### Set up data to call in plot
  # Refinement parameter
  size <- 250
  
  # Model settings
  pi <- rep(seq(0,1,length=size), 9)
  
  # Prior parameters
  a <- rep(as, each = size*3)
  b <- rep(bs, each = size*3)
  # Data
  x <- rep(rep(xs, each = size), 3)
  n <- rep(rep(ns, each = size), 3)
  # Posterior parameters
  a_post <- x + a
  b_post <- n - x + b
  # Labels
  setting_prior      <- as.factor(rep(1:3, each = size*3))
  setting_likelihood <- as.factor(rep(rep(1:3, each = size), 3))
  levels(setting_prior)      <- labs_prior
  levels(setting_likelihood) <- labs_likelihood    
  # Prior, likelihood, posterior functions
  bfun1 <- function(x){dbinom(x = xs[1], size = ns[1], prob = x)}
  bfun2 <- function(x){dbinom(x = xs[2], size = ns[2], prob = x)}
  bfun3 <- function(x){dbinom(x = xs[3], size = ns[3], prob = x)}
  scale   <- rep(rep(c(integrate(bfun1, 0, 1)[[1]], integrate(bfun2, 0, 1)[[1]], integrate(bfun3, 0, 1)[[1]]), each = size), 3)
  prior      <- dbeta(x = pi, shape1 = a, shape2 = b)
  likelihood <- dbinom(x = x, size = n, prob = pi) / scale
  posterior  <- dbeta(x = pi, shape1 = a_post, shape2 = b_post)
  # Combine into data frame
  data.frame(setting_prior, setting_likelihood, pi, a, b, x, n, likelihood, prior, posterior)
}
plot_dat <- make_plot_data(
  as = c(5,1,14), bs = c(11,1,1), 
  xs = c(6,29,46), ns = c(13,63,99), 
  labs_prior = c("prior: Beta(5,11)", "prior: Beta(1,1)", "prior: Beta(14,1)"), 
  labs_likelihood = c("data: Y = 6 of n = 13", "data: Y = 29 of n = 63", "data: Y = 46 of n = 99"))
```


```{r echo = FALSE, fig.align='center', cache=TRUE}
plot_dat_new <- plot_dat %>% 
  mutate(setting_prior = factor(setting_prior, 
                                levels = c("prior: Beta(14,1)", "prior: Beta(5,11)", "prior: Beta(1,1)")))
beta_binom_grid_plot(plot_dat_new, posterior = TRUE, likelihood = TRUE) + 
  facet_grid(setting_prior ~ setting_likelihood) +
  theme(text = element_text(size=17)) 
```

##

:::: {.columns}

::: {.column width="50%"}
### Plotting Functions

`plot_beta()`   
`plot_binomial_likelihood()`  
`plot_beta_binomial`  

`plot_gamma()`  
`plot_poisson_likelihood()`  
`plot_gamma_poisson()`  

`plot_normal()`  
`plot_normal_likelihood()`  
`plot_normal_normal()`  

:::

::: {.column width="50%"}
### Summary Functions

`summarize_beta()`
`summarize_beta_binomial()`  

<br>

`summarize_gamma()`
`summarize_gamma_poisson()`  

<br>


`summarize_normal_normal()`



:::

::::

## Model Evaluation Functions

| Functions                                                        | Response     | Model Type |
|------------------------------------------------------------------|--------------|------------|
| `prediction_summary()` <br> `prediction_summary_cv()`                     | Quantitative | rstanreg   |
| `classification_summary()` `classification_summary_cv()`             | Binary       | rstanreg   |
| `naive_classification_summary()` `naive_classification_summary_cv()` | Categorical  | naiveBayes |



## Prediction Summary


```{r echo=FALSE, cache=TRUE}
data <- data.frame(x = sample(1:100, 20)) 
data$y <- data$x*3 + rnorm(20, 0, 5)
model <- rstanarm::stan_glm(y ~ x,  data =data, refresh = FALSE, seed=84735)
```



:::: {.columns}

::: {.column width="50%"}

```{r cache=TRUE}
prediction_summary(model, data, 
                   prob_inner = 0.6, 
                   prob_outer = 0.80)
```

:::

::: {.column width="50%"}
```{r cache=TRUE}
prediction_summary_cv(model = model, 
                      data = data, 
                      k = 2, #<<
                      prob_inner = 0.6, 
                      prob_outer = 0.80)
```
:::

::::

## `library(rstan)`

:::: {.columns}

::: {.column width="50%"}
```{r}
# STEP 1: DEFINE the model
stan_bike_model <- "
  data {
    int<lower=0> n;
    vector[n] Y;
    vector[n] X;
  }
  parameters {
    real beta0;
    real beta1;
    real<lower=0> sigma;
  }
  model {
    Y ~ normal(beta0 + beta1 * X, sigma);
  }
"
```

:::

::: {.column width="50%"}
```{r eval=FALSE}
# STEP 2: SIMULATE the posterior
stan_bike_sim <- 
  stan(model_code = stan_bike_model, 
  data = list(n = nrow(bikes), 
              Y = bikes$rides, X = bikes$temp_feel), 
  chains = 4, iter = 5000*2, seed = 84735)
```
:::

::::

## `library(rstanarm)`


```{r eval=FALSE}
normal_model_sim <- stan_glm(rides ~ temp_feel, 
                             data = bikes, 
                             family = gaussian, 
                             chains = 4, iter = 5000*2,
                             seed = 84735)
```

```{r cache=TRUE, echo=FALSE}
normal_model_sim <- 
  stan_glm(rides ~ temp_feel, 
           data = bikes, 
           family = gaussian, 
           chains = 4, iter = 5000*2,
           seed = 84735,
           refresh = FALSE)
```

## `library(bayesplot)`

:::: {.columns}

::: {.column width="50%"}

```{r out.width = "80%", fig.align='center'}
mcmc_trace(normal_model_sim, size = 0.1)
```

:::

::: {.column width="50%"}

```{r out.width = "80%", fig.align='center'}
mcmc_dens_overlay(normal_model_sim)
```

:::

::::

## Resources

- [Undergraduate Bayesian Education Resources](https://undergrad-bayes.netlify.app/)



- [Undergraduate Bayesian Education Network](https://undergrad-bayes.netlify.app/network.html)



- [STATS 115 at UC Irvine](https://www.stats115.com)


## QUESTIONS?

`r fontawesome::fa(name = "link")`
<a href = "http://minedogucu.com">minedogucu.com</a>   

`r fontawesome::fa(name = "github")` <a href = "http://github.com/mdogucu">mdogucu</a> 

`r fontawesome::fa(name = "bluesky")` <a href = "https://bsky.app/profile/minedogucu.com">minedogucu.com</a> 

`r fontawesome::fa(name = "mastodon")` <a href = "https://mastodon.social/@MineDogucu">mastodon.social/@MineDogucu</a> 

`r fontawesome::fa(name = "linkedin")` <a href = "https://www.linkedin.com/in/minedogucu/">minedogucu</a> 